{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ6wc2HE0pke"
   },
   "source": [
    "# **Experiment Notebook**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qNOA146K2c6f"
   },
   "outputs": [],
   "source": [
    "# Do not modify this code\n",
    "!pip install -q utstd\n",
    "\n",
    "from utstd.ipyrenders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6FneOmBfka9G"
   },
   "outputs": [],
   "source": [
    "# Do not modify this code\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXFKfa2tp1ch"
   },
   "source": [
    "## 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GBEAwdncnlAx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQgxLRrvjiJb"
   },
   "source": [
    "---\n",
    "## A. Project Description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Je1EzzfFD5hj"
   },
   "outputs": [],
   "source": [
    "student_name = \"Fang Yee Tan\"\n",
    "student_id = \"25677648\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pdKiYvFWD5my"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">student_name</p><h1 font-size: 3em>Fang Yee Tan</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h1\", key='student_name', value=student_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9KTEbRjqD5o_"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">student_id</p><h1 font-size: 3em>25677648</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h1\", key='student_id', value=student_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0zsEPshwy1K"
   },
   "source": [
    "---\n",
    "## C. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGMWhKSbUl63"
   },
   "source": [
    "### C.1   Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NKgOzSn-w0eq"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "file_path = Path.home() / \"Desktop\" / \"36120\"\n",
    "\n",
    "X_train = pd.read_csv(file_path / \"X_train.csv\")\n",
    "X_val = pd.read_csv(file_path / \"X_val.csv\")\n",
    "X_test = pd.read_csv(file_path / \"X_test.csv\")\n",
    "y_train = pd.read_csv(file_path / \"y_train.csv\").squeeze()\n",
    "y_val = pd.read_csv(file_path / \"y_val.csv\").squeeze()\n",
    "y_test = pd.read_csv(file_path / \"y_test.csv\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtLjr7niHpNZ"
   },
   "source": [
    "---\n",
    "## I. Selection of Performance Metrics\n",
    "\n",
    "> Provide some explanations on why you believe the performance metrics you chose is appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KV_pxLAiHxKW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kC949nluHR5s"
   },
   "outputs": [],
   "source": [
    "performance_metrics_explanations = \"\"\"Root Mean Squared Error (RMSE) and R squared (R²) are selected as the primary performance metrics for this project. RMSE is chosen because it shares the same units as the target variable, precipitation in millimeters, making the magnitude of errors straightforward and easy to interpret. Furthermore, RMSE penalises larger errors more heavily, which is crucial for accurately reflecting the impact of significant deviations in precipitation forecasts. R squared is included as it quantifies the proportion of variance in the observed data explained by the model, providing valuable insight into the overall goodness of fit and facilitating a comprehensive evaluation of model performance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wABRzU2sHR8j"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">performance_metrics_explanations</p><h3 font-size: 3em>Root Mean Squared Error (RMSE) and R squared (R²) are selected as the primary performance metrics for this project. RMSE is chosen because it shares the same units as the target variable, precipitation in millimeters, making the magnitude of errors straightforward and easy to interpret. Furthermore, RMSE penalises larger errors more heavily, which is crucial for accurately reflecting the impact of significant deviations in precipitation forecasts. R squared is included as it quantifies the proportion of variance in the observed data explained by the model, providing valuable insight into the overall goodness of fit and facilitating a comprehensive evaluation of model performance.</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h3\", key='performance_metrics_explanations', value=performance_metrics_explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpxjwSDYIJy6"
   },
   "source": [
    "## J. Train Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis: There is no statistically significant difference in predictive performance between the linear regression model and the baseline model.\n",
    "\n",
    "Alternative Hypothesis: There is a statistically significant difference in predictive performance between the linear regression model and the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XBy7-9PIVcU"
   },
   "source": [
    "### J.1 Import Algorithm\n",
    "\n",
    "> Provide some explanations on why you believe this algorithm is a good fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "diUB08xMIOuS"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GIWOpv6CGUTE"
   },
   "outputs": [],
   "source": [
    "algorithm_selection_explanations = \"\"\"Ridge, Lasso and ElasticNet regression models are employed in this project as they are relatively simple yet effective approaches for addressing multicollinearity and high dimensionality. Ridge regression utilises L2 regularisation to shrink coefficient values without eliminating predictors, thereby reducing overfitting and improving model stability when input features are highly correlated. Lasso regression incorporates L1 regularisation, which not only shrinks coefficients but also performs variable selection by setting some coefficients exactly to zero, enhancing interpretability and reducing the influence of less relevant variables. ElasticNet combines both L1 and L2 regularisation, making it particularly effective in scenarios involving numerous and correlated predictors, as it leverages the strengths of both Ridge and Lasso. Collectively, these models offer a strong foundation for developing precipitation prediction models that are interpretable, generalisable, and robust.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N4yogQ9aGUVe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">algorithm_selection_explanations</p><h3 font-size: 3em>Ridge, Lasso and ElasticNet regression models are employed in this project as they are relatively simple yet effective approaches for addressing multicollinearity and high dimensionality. Ridge regression utilises L2 regularisation to shrink coefficient values without eliminating predictors, thereby reducing overfitting and improving model stability when input features are highly correlated. Lasso regression incorporates L1 regularisation, which not only shrinks coefficients but also performs variable selection by setting some coefficients exactly to zero, enhancing interpretability and reducing the influence of less relevant variables. ElasticNet combines both L1 and L2 regularisation, making it particularly effective in scenarios involving numerous and correlated predictors, as it leverages the strengths of both Ridge and Lasso. Collectively, these models offer a strong foundation for developing precipitation prediction models that are interpretable, generalisable, and robust.</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h3\", key='algorithm_selection_explanations', value=algorithm_selection_explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ks_MmM2mCfm"
   },
   "source": [
    "### J.2 Set Hyperparameters\n",
    "\n",
    "> Provide some explanations on why you believe this algorithm is a good fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series cross valaidation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Models and hyperparameters\n",
    "models = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {'model__alpha': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': Lasso(max_iter=100000),\n",
    "        'params': {'model__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'model': ElasticNet(max_iter=100000),\n",
    "        'params': {\n",
    "            'model__alpha': [0.001, 0.01, 0.1, 1, 10],\n",
    "            'model__l1_ratio': [0.1, 0.5, 0.9, 1]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "crG_Bm72HfL-"
   },
   "outputs": [],
   "source": [
    "hyperparameters_selection_explanations = \"\"\"The alpha parameter is tuned to control the strength of regularisation across all three models. Regularisation helps prevent overfitting by penalising large coefficient values, thereby improving the model’s ability to generalise to unseen data. In the case of ElasticNet, an additional hyperparameter, l1_ratio, determines the balance between L1 and L2 regularisation. When l1_ratio is close to 0, ElasticNet behaves more like Ridge regression by applying primarily L2 regularisation. Conversely, when l1_ratio approaches 1, it acts more like Lasso regression, placing greater emphasis on L1 regularisation. Tuning these parameters enables the model to optimise its regularisation strategy, ultimately improving predictive performance.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "k0uqlRCVHfQI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">hyperparameters_selection_explanations</p><h3 font-size: 3em>The alpha parameter is tuned to control the strength of regularisation across all three models. Regularisation helps prevent overfitting by penalising large coefficient values, thereby improving the model’s ability to generalise to unseen data. In the case of ElasticNet, an additional hyperparameter, l1_ratio, determines the balance between L1 and L2 regularisation. When l1_ratio is close to 0, ElasticNet behaves more like Ridge regression by applying primarily L2 regularisation. Conversely, when l1_ratio approaches 1, it acts more like Lasso regression, placing greater emphasis on L1 regularisation. Tuning these parameters enables the model to optimise its regularisation strategy, ultimately improving predictive performance.\n",
       "</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h3\", key='hyperparameters_selection_explanations', value=hyperparameters_selection_explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDjdjQjFmkLe"
   },
   "source": [
    "### J.3 Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0Ub3Nrdgmm2N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GridSearchCV for Ridge\n",
      "Best params: {'model__alpha': 100}\n",
      "Best RMSE: 14.3880\n",
      "Best R2: 0.0936\n",
      "\n",
      "GridSearchCV for Lasso\n",
      "Best params: {'model__alpha': 0.01}\n",
      "Best RMSE: 14.3940\n",
      "Best R2: 0.0914\n",
      "\n",
      "GridSearchCV for ElasticNet\n",
      "Best params: {'model__alpha': 0.01, 'model__l1_ratio': 0.9}\n",
      "Best RMSE: 14.3937\n",
      "Best R2: 0.0915\n"
     ]
    }
   ],
   "source": [
    "scoring = {\n",
    "    'rmse': 'neg_root_mean_squared_error',\n",
    "    'r2': 'r2'\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\nGridSearchCV for {name}\")\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('model', config['model'])\n",
    "    ])\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=config['params'],\n",
    "        cv=tscv,\n",
    "        scoring=scoring,\n",
    "        refit='rmse',  \n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_rmse = -grid.best_score_  \n",
    "    best_r2 = grid.cv_results_['mean_test_r2'][grid.best_index_]\n",
    "    \n",
    "    print(f\"Best params: {grid.best_params_}\")\n",
    "    print(f\"Best RMSE: {best_rmse:.4f}\")\n",
    "    print(f\"Best R2: {best_r2:.4f}\")\n",
    "    \n",
    "    best_models[name] = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q43YtqpdeniY"
   },
   "source": [
    "### J.4 Model Technical Performance\n",
    "\n",
    "> Provide some explanations on model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with best Ridge model:\n",
      "Ridge Validation RMSE: 11.0580\n",
      "Ridge Validation R2: 0.0019\n",
      "\n",
      "Predicting with best Lasso model:\n",
      "Lasso Validation RMSE: 11.1443\n",
      "Lasso Validation R2: -0.0137\n",
      "\n",
      "Predicting with best ElasticNet model:\n",
      "ElasticNet Validation RMSE: 11.1381\n",
      "ElasticNet Validation R2: -0.0126\n"
     ]
    }
   ],
   "source": [
    "# Validation dataset\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nPredicting with best {name} model:\")\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"{name} Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"{name} Validation R2: {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with best Ridge model:\n",
      "Ridge Testing RMSE: 13.9550\n",
      "Ridge Testing R2: 0.1032\n",
      "\n",
      "Predicting with best Lasso model:\n",
      "Lasso Testing RMSE: 14.0800\n",
      "Lasso Testing R2: 0.0870\n",
      "\n",
      "Predicting with best ElasticNet model:\n",
      "ElasticNet Testing RMSE: 14.0684\n",
      "ElasticNet Testing R2: 0.0885\n"
     ]
    }
   ],
   "source": [
    "# Testing dataset\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nPredicting with best {name} model:\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"{name} Testing RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"{name} Testing R2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "_YbQldshHk_3"
   },
   "outputs": [],
   "source": [
    "model_performance_explanations = \"\"\"From the performance metrics on both the validation and testing datasets, it is evident that the models struggle to effectively learn the underlying patterns. The predictive performance of Ridge, Lasso and ElasticNet regression models is poor, with R² scores around 0.08 on the testing dataset, indicating that each model explains only about 8% of the variance in cumulative precipitation over the next three days. This limited performance suggests that these linear models are unable to capture the full complexity of precipitation patterns. For ElasticNet, the optimal l1_ratio of 0.9 reflects a strong preference for L1 regularisation, which encourages sparsity by shrinking some coefficients to zero. However, this also highlights that these models may be too simplistic for the dataset, struggling to capture nonlinear relationships and complex interactions likely present in precipitation data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "-MkLnLzVHlDO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">model_performance_explanations</p><h3 font-size: 3em>From the performance metrics on both the validation and testing datasets, it is evident that the models struggle to effectively learn the underlying patterns. The predictive performance of Ridge, Lasso and ElasticNet regression models is poor, with R² scores around 0.08 on the testing dataset, indicating that each model explains only about 8% of the variance in cumulative precipitation over the next three days. This limited performance suggests that these linear models are unable to capture the full complexity of precipitation patterns. For ElasticNet, the optimal l1_ratio of 0.9 reflects a strong preference for L1 regularisation, which encourages sparsity by shrinking some coefficients to zero. However, this also highlights that these models may be too simplistic for the dataset, struggling to capture nonlinear relationships and complex interactions likely present in precipitation data.\n",
       "</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h3\", key='model_performance_explanations', value=model_performance_explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1HgZMPcmtu7"
   },
   "source": [
    "### J.5 Business Impact from Current Model Performance\n",
    "\n",
    "> Provide some analysis on the model impacts from the business point of view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7bcCHiP-Hozj"
   },
   "outputs": [],
   "source": [
    "business_impacts_explanations = \"\"\"Since the three models explain only about 9% of the variance in precipitation and have an RMSE of approximately 14 millimeters, their predictive accuracy is limited. From a business perspective, such errors could have significant consequences. Inaccurate rainfall forecasts may lead to poor decision-making in industries that depend heavily on weather conditions. Overestimating rainfall might result in unnecessary resource allocation, leading to increased operational costs. Conversely, underestimating precipitation could leave businesses unprepared for heavy rain, causing operational disruptions, financial losses, and, more importantly, potential safety risks for personnel and the public.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RQ3lJGAnHo3O"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">business_impacts_explanations</p><h3 font-size: 3em>Since the three models explain only about 9% of the variance in precipitation and have an RMSE of approximately 14 millimeters, their predictive accuracy is limited. From a business perspective, such errors could have significant consequences. Inaccurate rainfall forecasts may lead to poor decision-making in industries that depend heavily on weather conditions. Overestimating rainfall might result in unnecessary resource allocation, leading to increased operational costs. Conversely, underestimating precipitation could leave businesses unprepared for heavy rain, causing operational disruptions, financial losses, and, more importantly, potential safety risks for personnel and the public.\n",
       "</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h3\", key='business_impacts_explanations', value=business_impacts_explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp1Ie9o8nDl1"
   },
   "source": [
    "## H. Project Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AvFNheh1HtPb"
   },
   "outputs": [],
   "source": [
    "experiment_outcome = \"\"\"Hypothesis Rejected\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "23bFWNIVHtTU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">experiment_outcomes_explanations</p><h2 font-size: 3em>Hypothesis Rejected</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h2\", key='experiment_outcomes_explanations', value=experiment_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "iFF8wsz6HteA"
   },
   "outputs": [],
   "source": [
    "experiment_results_explanations = \"\"\"The experiment results indicate that the current linear regression models, Ridge, Lasso and ElasticNet, are only capturing about 9% of the variance in precipitation, with relatively high RMSE values around 14 millimeters. This suggests that the relationship between the features and precipitation is likely nonlinear and more complex than what simple linear models can capture. As a result, it is clear that more advanced models such as Random Forest and Gradient Boosting Machines should be explored next, as they are better suited to model nonlinear interactions and can potentially improve predictive performance significantly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "QpWzEhX2HthW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color:grey\">experiment_results_explanations</p><h2 font-size: 3em>The experiment results indicate that the current linear regression models, Ridge, Lasso and ElasticNet, are only capturing about 9% of the variance in precipitation, with relatively high RMSE values around 14 millimeters. This suggests that the relationship between the features and precipitation is likely nonlinear and more complex than what simple linear models can capture. As a result, it is clear that more advanced models such as Random Forest and Gradient Boosting Machines should be explored next, as they are better suited to model nonlinear interactions and can potentially improve predictive performance significantly.\n",
       "</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not modify this code\n",
    "print_tile(size=\"h2\", key='experiment_results_explanations', value=experiment_results_explanations)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "13MHsQlFmfLuY1rWjvW73WSaDUaRPENW4",
     "timestamp": 1680399055727
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
